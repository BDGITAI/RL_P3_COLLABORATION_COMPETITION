{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "## Preamble\n",
    "\n",
    "### The project\n",
    "\n",
    "In this notebook, an agent based on the MADDPG algorithm is used to solve a Tennis Game for the  [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "Before using this notebook check that you have followed the .Readme file available in [GitHub Project repository](https://github.com/BDGITAI/RL_P3_COLLABORATION_COMPETITION)\n",
    "\n",
    "For the Notebook to work you will need the Reacher environment executable which was placed in [GitHub Project repository](https://github.com/BDGITAI/RL_P3_COLLABORATION_COMPETITION/Tennis_Windows_x86_64/). The environment needs to uncompressed as  `\"./Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "\n",
    "This implementation uses the Pytorch library and was tested in a **Windows 64 bits** platform using **CPU**  computation. \n",
    "\n",
    "\n",
    "This notebook is divided in two parts\n",
    "* **Part 1** : Training. We will train an Agent and see how the learning can be applied to execute a task\n",
    "* **Part 2** : To see an already trained agent you can skip to Part 2 and load a trained agent.\n",
    "\n",
    "\n",
    "### Base used for the project\n",
    "\n",
    "Some files used in the project are based on a starter code provide in the nanodegree to solve the pendulum openai gym.\n",
    "Modifications of original files are indicated in the comments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Part 1 : Training an agent \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of required librairies first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required librairies\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a function used to pre fill a memory buffer. This function was created to reinforce the positive reward signal and try to stabilise the learning. All actions are taken with a random policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "#\n",
    "#   Function to pre initialise the experience buffer of the MADDPG agent\n",
    "#   Use to try to reinforce the positive reward signal during learning \n",
    "#   When not trained the agent receives mostly neutral or negative rewards\n",
    "#   Introduced this function to use random experiences to boot strap the memory \n",
    "#   Different ratio used to control the filling of the buffer. \n",
    "#   For instance half of the memory is filled with 20% of positive exp and then 40% neg, 40% 0\n",
    "#\n",
    "#####################################################################################################\n",
    "\n",
    "def collect_experience(agent,number_of_episodes):\n",
    "    \"\"\"Load saved network param\n",
    "        Params\n",
    "        ======\n",
    "            agent (a MADDPG agent): MADDPG agent that is bootstrapped\n",
    "            number_of_episodes (int): number of episodes used to fill the memory\n",
    "    \"\"\" \n",
    "    # create a unity environment\n",
    "    env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    # define filling ratio\n",
    "    # e.g. : 20 % of positive reward = 0.2\n",
    "    pos_ratio = 0.2\n",
    "    neg_ratio = 0.4\n",
    "    neutral_ratio = 0.4\n",
    "    # counter for number of experiences of each type\n",
    "    pos_actual = 0\n",
    "    neg_actual = 0\n",
    "    neu_actual = 0\n",
    "    # amount of memory to be filled\n",
    "    memory_fill_ration = 0.5\n",
    "    to_fill = agent.buffer_size*memory_fill_ration\n",
    "    \n",
    "    # do random actions for the number of episode given\n",
    "    for i_episode in range(1, number_of_episodes+1):\n",
    "        # reset at each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] \n",
    "        # get all agents states\n",
    "        states = env_info.vector_observations\n",
    "        while True:\n",
    "            actions = np.random.randn(2, 2)              # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)            # all actions between -1 and 1            \n",
    "            env_info = env.step(actions)[brain_name]     # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished   \n",
    "            \n",
    "            # add data to buffer if the size is inferior to threshold\n",
    "            if len(agent.memory)< to_fill:\n",
    "                # reshape data to be compatible with buffer \n",
    "                state = states.reshape(1, -1)  \n",
    "                next_state = next_states.reshape(1, -1)  \n",
    "                action= np.array(actions).reshape(1, -1)\n",
    "                # if current rewards are positive store if under the ratio\n",
    "                if np.mean(rewards)>0 and (pos_actual/to_fill)<=pos_ratio:\n",
    "                    agent.memory.add(state, action, rewards, next_state, dones)\n",
    "                    #update counter\n",
    "                    pos_actual+=1\n",
    "                if np.mean(rewards)<0 and (neg_actual/to_fill)<=neg_ratio:\n",
    "                    agent.memory.add(state, action, rewards, next_state, dones)\n",
    "                    neg_actual+=1\n",
    "                if ((np.mean(rewards)==0) and ((neu_actual/to_fill)<=neutral_ratio)):\n",
    "                    agent.memory.add(state, action, rewards, next_state, dones)\n",
    "                    neu_actual+=1\n",
    "            #update state for next time step\n",
    "            states = next_states\n",
    "            # if episode is finished\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        # monitoring values\n",
    "        actual_pos = pos_actual/to_fill\n",
    "        neg_pos = neg_actual/to_fill\n",
    "        neu_pos = neu_actual/to_fill\n",
    "        total = actual_pos + neg_pos + neu_pos\n",
    "        # display fillinf progress\n",
    "        print('\\rEpisode {}\\tPos: {:.4f}\\tNeg: {:.4f}\\tNeu: {:.4f}\\tFill :{}'.format(i_episode, actual_pos,neg_pos,neu_pos,to_fill), end=\"\")\n",
    "        if len(agent.memory)== to_fill:\n",
    "            break\n",
    "    # close unity environment\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop that can be used with or without the pre fill function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "#\n",
    "#   Training loop \n",
    "#\n",
    "#####################################################################################################    \n",
    "\n",
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "def train(agent, number_of_episodes = 1):\n",
    "    \"\"\"Load saved network param\n",
    "        Params\n",
    "        ======\n",
    "            agent (a MADDPG agent): MADDPG agent that is bootstrapped\n",
    "            number_of_episodes (int): number of episodes used to fill the memory\n",
    "    \"\"\" \n",
    "    seeding()\n",
    "    # save initial weights\n",
    "    # agent.save('init')\n",
    "    # monitor current best score to solve networks even if final score not achieved\n",
    "    best_mean_score = -1\n",
    "\n",
    "    # build environment\n",
    "    env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    #\n",
    "    scores = []     # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    \n",
    "    # training loop    \n",
    "    for i_episode in range(1, number_of_episodes+1):\n",
    "        # init rewards for all agents\n",
    "        reward_this_episode = np.zeros(len(agent.ddpg_agents))\n",
    "        #reset env for each episode\n",
    "        env_info = env.reset(train_mode=True)[brain_name] \n",
    "        states = env_info.vector_observations\n",
    "        while True:\n",
    "            # decide actions\n",
    "            actions = agent.act(states, add_noise=True)\n",
    "          \n",
    "            # step forward \n",
    "            env_info = env.step(actions)[brain_name]     # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished   \n",
    "            \n",
    "            # add data to buffer\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            # cumul reward\n",
    "            reward_this_episode += rewards\n",
    "            \n",
    "            #update state for next time step\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break             \n",
    "                          \n",
    "        # compute score \n",
    "        # use max amongst all agents as directed in project instructions\n",
    "        score = np.max(reward_this_episode)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        # display progress\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}\\tMax Score: {:.4f}\\tReward this episode: {}'.format(i_episode, np.mean(scores_window),np.max(scores_window),reward_this_episode), end=\"\")\n",
    "\n",
    "        # print every 100 episodes and save if average better than previous\n",
    "        if i_episode % 100 == 0 or i_episode == number_of_episodes-1:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if best_mean_score < np.mean(scores_window):\n",
    "                agent.save('best_achieved')\n",
    "                best_mean_score = np.mean(scores_window)\n",
    "        # save and end if score is achieved      \n",
    "        if np.mean(scores_window) >= 0.5:\n",
    "            print('\\rSolved Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            agent.save('solved')\n",
    "            break\n",
    "    # save where training stopped if target score is not achieved\n",
    "    if np.mean(scores_window) < 0.5:        \n",
    "        agent.save('end')\n",
    "    # close unity\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of agent and prefill of the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000\tPos: 0.1487\tNeg: 0.4001\tNeu: 0.4001\tFill :15000.0"
     ]
    }
   ],
   "source": [
    "# create agent\n",
    "# Observations dim for each agent = 24\n",
    "# action dim = 2\n",
    "# create 2 agents\n",
    "agent = MADDPG(24, 2, 2, buffer_size=int(1e5), batch_size=128, discount_factor=0.99, tau=0.001)\n",
    "\n",
    "# use this line to prefill the memory \n",
    "collect_experience(agent,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.0000\tMax Score: 0.0000\tReward this episode: [ 0.   -0.01]\n",
      "Episode 200\tAverage Score: 0.0000\tMax Score: 0.0000\tReward this episode: [ 0.   -0.01]\n",
      "Episode 300\tAverage Score: 0.0029\tMax Score: 0.2000\tReward this episode: [ 0.    0.09]\n",
      "Episode 400\tAverage Score: 0.0252\tMax Score: 0.1000\tReward this episode: [ 0.   -0.01]\n",
      "Episode 500\tAverage Score: 0.0000\tMax Score: 0.0000\tReward this episode: [ 0.   -0.01]\n",
      "Episode 600\tAverage Score: 0.0000\tMax Score: 0.0000\tReward this episode: [ 0.   -0.01]\n",
      "Episode 700\tAverage Score: 0.0000\tMax Score: 0.0000\tReward this episode: [-0.01  0.  ]\n",
      "Episode 800\tAverage Score: 0.0018\tMax Score: 0.0900\tReward this episode: [ 0.   -0.01]\n",
      "Episode 900\tAverage Score: 0.0350\tMax Score: 0.2000\tReward this episode: [ 0.    0.09]\n",
      "Episode 1000\tAverage Score: 0.0766\tMax Score: 0.3000\tReward this episode: [ 0.1  -0.01]\n",
      "Episode 1100\tAverage Score: 0.1082\tMax Score: 0.2000\tReward this episode: [ 0.    0.09]\n",
      "Episode 1200\tAverage Score: 0.1285\tMax Score: 0.5000\tReward this episode: [ 0.39000001  0.40000001]\n",
      "Episode 1300\tAverage Score: 0.1315\tMax Score: 0.6000\tReward this episode: [ 0.2   0.09] 0.49000001]\n",
      "Episode 1400\tAverage Score: 0.1285\tMax Score: 0.4000\tReward this episode: [ 0.1  -0.01] 0.40000001]\n",
      "Episode 1500\tAverage Score: 0.2326\tMax Score: 1.7000\tReward this episode: [ 0.80000001  0.69000001]\n",
      "Episode 1600\tAverage Score: 0.0549\tMax Score: 0.9000\tReward this episode: [ 0.   -0.01] 0.40000001]\n",
      "Episode 1700\tAverage Score: 0.1065\tMax Score: 1.1000\tReward this episode: [-0.01  0.1 ] 0.99000002]\n",
      "Solved Episode 1768\tAverage Score: 0.52Score: 2.7000\tReward this episode: [ 2.60000004  2.60000004]\n"
     ]
    }
   ],
   "source": [
    "# perform the training\n",
    "train(agent,number_of_episodes = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part 2 : Watch a trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import librairies if no pre training done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required librairies\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "#\n",
    "#   Perform an evaluation in non training mode \n",
    "#\n",
    "#####################################################################################################  \n",
    "\n",
    "def evaluate(agent,number_of_episodes = 10):\n",
    "    #\n",
    "    env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "        \n",
    "    # path to saved weights\n",
    "    path = []\n",
    "    path.append('./successful_weigths/solved1.actor.pth')\n",
    "    path.append('./successful_weigths/solved0.actor.pth')\n",
    "   \n",
    "    # load networks\n",
    "    agent.load(path)\n",
    "    \n",
    "    #\n",
    "    scores = []     # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "    for i_episode in range(1, number_of_episodes+1):\n",
    "        \n",
    "        reward_this_episode = np.zeros(2)\n",
    "        # reset environment not in training mode\n",
    "        env_info = env.reset(train_mode=False)[brain_name] \n",
    "        states = env_info.vector_observations\n",
    "        while True:\n",
    "            # noise disables in evaluation\n",
    "            actions = agent.act(states, add_noise=False)\n",
    "            # step forward one frame\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished                    \n",
    "            reward_this_episode += rewards\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        # save scores\n",
    "        score = np.max(reward_this_episode)\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.4f}\\tMax Score: {:.4f}\\tReward this episode: {}'.format(i_episode, np.mean(scores_window),np.max(scores_window),reward_this_episode), end=\"\")\n",
    "    env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 10\tAverage Score: 1.3590\tMax Score: 2.7000\tReward this episode: [ 0.1   0.09]"
     ]
    }
   ],
   "source": [
    "agent = MADDPG(24, 2, 2)\n",
    "evaluate(agent,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
